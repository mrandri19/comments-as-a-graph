digraph {
	node [color="#B7C5D9" fillcolor="#F6F6EF" fontname="helvetica, open-sans" shape=rectangle style=filled]
	edge [dir=back]
	bgcolor="#eef2ff" fontname="helvetica, open-sans" splines=true
	16240983 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>There is a public distributed effort happening for Go right now:<BR/>[http://zero.sjeng.org/](http://zero.sjeng.org/). They've been doing a<BR/>fantastic job, and just recently fixed a big training bug that has resulted in<BR/>a large strength increase.<BR/><BR/>I ported over from GCP's Go implementation to chess:<BR/>[https://github.com/glinscott/leela-chess](https://github.com/glinscott/leela-<BR/>chess). The distributed part isn't ready to go yet, we are still working the<BR/>bugs out using supervised training, but will be launching soon!<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240673 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Can someone share some intuition of the tradeoffs between monte-carlo tree<BR/>search compared to vanilla policy gradient reinforcement learning?<BR/><BR/>MCTS has gotten really popular as of AlphaZero, but it's not clear to me how<BR/>this compares to more simple reinforcement learning techniques that just have<BR/>a softmax output of the possible moves the agent can make. My intuition is<BR/>that MCTS is better for planning, but takes longer to train/evaluate. Is that<BR/>true? Is there some games one will work better than the other?<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241820 -> 16240673
	16241820 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>The MCTS variant used by AZ offers a big advantage in the<BR/>exploration/exploitation tradeoff, with a better exploration profile during<BR/>training and better exploitation during competitive play.<BR/><BR/>The training phase emphasizes exploration using a modified upper confidence<BR/>bound for tree search criteria to limit the expansion of the search tree. With<BR/>a uniform prior over the action space (like standard MCTS), you will explore a<BR/>large number of unlikely actions. The prior provided by the NN allows them to<BR/>bootstrap the value of leaf nodes _and_ efficiently sample the actions at each<BR/>state. AZ has eliminated the simulation phase of MCTS entirely.<BR/><BR/>AZ uses different criteria for move selection during training and during<BR/>competitive play. The competitive selection criteria basically replaces the<BR/>UCBT criteria with standard argmax over the actions (like a normal RL policy<BR/>selection). We know that tree pruning can be very effective during search _if_<BR/>you can order the nodes in a beneficial way. AZ MCTS uses the output of the NN<BR/>as the prior instead of a uniform prior, which is like a "virtual" pruning<BR/>(because moves with low probability in the prior are unlikely to be explored).<BR/>This tends to more quickly focus the search on strong lines of play, so the<BR/>algorithm produces strong choices with less search (AZ uses about an order of<BR/>magnitude fewer expansions than other state-of-the-art MCTS engines).<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240847 -> 16240673
	16240847 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>In vanilla policy gradient, one plays the game to the end and then bumps the<BR/>probability of _all_ actions taken by the agent up (if AlphaGo won) or down<BR/>(if it lost). This is very slow because there ~150 moves in an expert game,<BR/>and we do not know which moves caused decisive victory or loss - i.e. the<BR/>problem of  "long term credit assignment". Also, it is actually preferable to<BR/>compute the policy gradient with respect to the _advantage_ of every action,<BR/>so we encourage actions that were better than average and punish actions that<BR/>were worse than average - otherwise the policy gradient estimator has high<BR/>variance.<BR/><BR/>I think about MCTS in the following way: suppose you have a perfect<BR/>"simulator" for some reinforcement learning task you are trying to accomplish<BR/>(i.e. real-world robot grasping for a cup). Then instead of trying to grasp<BR/>the cup over and over again, you can just try/"plan" in simulation until you<BR/>arrive at a motion plan that picks up the cup.<BR/><BR/>MCTS is exactly a "planning" module, and it works so well in Go because the<BR/>simulator fidelity is perfect. AlphaGo can't model adversary behavior<BR/>perfectly, but MCTS and the policy network complement each other because the<BR/>policy reduces the search space of MCTS. As long as the best adversary is not<BR/>far away from the space that MCTS + policy is able to consider, AlphaGo can<BR/>match or beat the adversary. Then, we train the value network to amortize the<BR/>computation of the MTCS operator (via Bellman equality). Finally, self-play is<BR/>an elegant solution for keeping adversary + policy close to each other.<BR/><BR/>For more rigorous mathematical intuition, Ferenc Huszar has a nice blog post<BR/>on MCTS as a "policy improvement operator": [http://www.inference.vc/alphago-<BR/>zero-policy-improvement-and-...](http://www.inference.vc/alphago-zero-policy-<BR/>improvement-and-vector-fields/)<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241005 -> 16240847
	16241005 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Thanks for the response! I'm familiar with how AlphaZero works, just primarily<BR/>curious about how performance/speed compare in situations with perfect<BR/>information and where it is possible to perfectly simulate<BR/>(pong/go/chess/etc).<BR/><BR/>I did not realize that MCTS helps with the credit assignment problem, that's<BR/>really interesting!<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240743 -> 16240673
	16240743 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>MCTS has always been popular and probably the best online/out-of-the-box MDP<BR/>technique. But fundamentally its different than RL methods. In MCTS you assume<BR/>you have a perfect simulation of the MDP which is rarely true outside of<BR/>games. MCTS also doesnt really have a learning phase.<BR/><BR/>&gt; My intuition is that MCTS is better for planning, but takes longer to<BR/>train/evaluate.<BR/><BR/>There is no training phase in MCTS, rollouts can take a while, its important<BR/>to have a fast simulator and rollout policy (like random/UCT).<BR/><BR/>&gt; Is there some games one will work better than the other?<BR/><BR/>Games with simulators and perfect information!<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240880 -> 16240743
	16240880 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>&gt; There is no training phase in MCTS<BR/><BR/>Sorry, I was referring specifically to AlphaZero's approach in which there is<BR/>training for the expert policies that guide the MCTS. And yes I'm assuming<BR/>there is perfect information and it can be simulated. Thanks for the response!<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240746 -> 16240673
	16240746 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>It has been a while since I read the paper, but I believe they have some<BR/>results where they ran AlphaZero without MCTS, using only the policy network.<BR/>My recollection is that is still performed pretty well, but it was clearly<BR/>outperformed by MCTS.<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240815 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>FYI: The AlphaGo documentary is now on Netflix.<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241833 -> 16240815
	16241833 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>And here is a much lower budget (but still enjoyable) documentary about Fine<BR/>Art, an AlphaGo clone by the Chinese company Tencent:<BR/><BR/>[https://www.youtube.com/watch?v=vHJ2BnFx8Ak](https://www.youtube.com/watch?v=vHJ2BnFx8Ak)<BR/><BR/>Favorite line from it: "AI blew the gates of the Go palace open. And we<BR/>realized there was no one inside."<BR/><BR/>Another interesting thought was when one of the developers gets asked why<BR/>bother to clone AlphaGo when AlphaGo already exists, he says something like<BR/>"Was it pointless for China to develop the atomic bomb even though the USA<BR/>already had?"<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241772 -> 16240815
	16241772 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Direct link:<BR/>[https://www.netflix.com/title/80190844](https://www.netflix.com/title/80190844)<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241809 -> 16241772
	16241809 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Thank you (and GP!) I know what I’ll be doing for the next 90 minutes.<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240726 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Shameless self plug. I spent a Saturday morning doing a similar (no monte-<BR/>carlo, no AI library) thing recently with tic-tac-toe. I based this mostly on<BR/>intuition, would love any feedback.<BR/><BR/>[https://github.com/frenchie4111/genetic-algorithm-<BR/>playground...](https://github.com/frenchie4111/genetic-algorithm-<BR/>playground/blob/master/tictactoe.ipynb)<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240879 -> 16240726
	16240879 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>I've only skimmed it a little bit so far, I'm just wondering, if both players<BR/>use the optimal strategy, shouldn't the game always result in a draw?<BR/><BR/>Which is why I was a bit confused by the target '80% Win/Tie rate going<BR/>second', but I could well be missing something.<BR/><BR/>Edit: I'm an idiot, I see that the opponent takes random moves now. Seems a<BR/>fun project :), a while ago I built a very simple rule-based tic-tac-toe thing<BR/>in lisp, but the rules were all hardcoded alas.<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241276 -> 16240726
	16241276 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>you'll have to resume your work better than this if you want any meaningful<BR/>feedback<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241832 -> 16241276
	16241832 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>So I’m not the person who posted the code, but I don’t think I really<BR/>understand what you mean but I want to learn from it. Can you explain what it<BR/>would mean in practical/layman terms to resume his work better?<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241129 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Thanks for the great demo! Uploaded to Azure Notebooks in case anyone wants to<BR/>run/play/edit...<BR/><BR/>[https://notebooks.azure.com/smortaz/libraries/Demo-<BR/>DeepReinf...](https://notebooks.azure.com/smortaz/libraries/Demo-<BR/>DeepReinforcementLearning)<BR/><BR/>Click Clone to get your own copy, then Run the run.ipynb file.<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241296 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>I like the title more if it’s “Roll your own Alpha zero using Keras and<BR/>python”<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241110 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>&gt; Not quite as complex as Go, but there are still 4,531,985,219,092 game<BR/>positions in total, so not trivial for a laptop to learn how to play well with<BR/>zero human input.<BR/><BR/>That's a small enough state space that it is indeed trivial to brute force it<BR/>on a laptop.<BR/><BR/>Putting aside that though, it would be interesting to compare vs a standard<BR/>alpha-beta pruning minimax algorithm running at various depth levels.<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240836 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>As an aside, does anybody know the monospace font that we see in the<BR/>screenshots? Here, for instance: [https://cdn-<BR/>images-1.medium.com/max/1200/1*8zfDGlLuXfiLGnWlz...](https://cdn-<BR/>images-1.medium.com/max/1200/1*8zfDGlLuXfiLGnWlzvZwmQ.png)<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241213 -> 16240836
	16241213 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>I don't think it's Office Code Pro but it looks very similar to me. Maybe that<BR/>helps you with the search!<BR/><BR/></FONT></TD></TR></TABLE>>]
	16241453 -> 16240836
	16241453 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Looks like SF Mono.<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240681 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Does anyone have a different link to this? It's insecure and Cisco keeps<BR/>blocking it, so I can't just proceed from Chrome.<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240693 -> 16240681
	16240693 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>The link looks https to me, but this is where it redirects if that's of any<BR/>use: [https://medium.com/applied-data-science/how-to-build-your-<BR/>ow...](https://medium.com/applied-data-science/how-to-build-your-own-<BR/>alphazero-ai-using-python-and-keras-7f664945c188)<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240690 -> 16240681
	16240690 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>Try this - [https://medium.com/applied-data-science/how-to-build-your-<BR/>ow...](https://medium.com/applied-data-science/how-to-build-your-own-<BR/>alphazero-ai-using-python-and-keras-7f664945c188)<BR/><BR/></FONT></TD></TR></TABLE>>]
	16240758 [label=<<TABLE ALIGN="LEFT" BORDER="0"><TR><TD BALIGN="LEFT"><FONT>If you actually want to contribute towards an open source AlphaZero<BR/>implementation you may want to checkout [https://github.com/gcp/leela-<BR/>zero](https://github.com/gcp/leela-zero)<BR/><BR/></FONT></TD></TR></TABLE>>]
}
